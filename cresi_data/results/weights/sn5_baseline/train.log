06-24 16:15 log      INFO     Training: weight_save_path: /content/drive/Shareddrives/CS482_Project/cresi_data/results/weights/sn5_baseline
06-24 16:15 log      INFO     len ds: 944
06-24 16:15 log      INFO     folds_file_loc: /content/drive/Shareddrives/CS482_Project/cresi_data/results/weights/sn5_baseline/folds5.csv
06-24 16:15 log      INFO     save_path: /content/drive/Shareddrives/CS482_Project/cresi_data/results/weights/sn5_baseline
06-24 16:15 log      INFO     num workers: 2
06-24 16:15 log      INFO     fold: 0
06-24 16:15 log      INFO     len(train_idx): 755
06-24 16:15 log      INFO     len(val_idx): 189
06-24 16:15 log      INFO     pytorch_utils train.py config.num_channels: 3
06-24 16:15 log      INFO     pytorch_utils train.py function train(),  model: Resnet34_upsample(
  (bottlenecks): ModuleList(
    (0): ConvBottleneck(
      (seq): Sequential(
        (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): ReLU(inplace=True)
      )
    )
    (1): ConvBottleneck(
      (seq): Sequential(
        (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): ReLU(inplace=True)
      )
    )
    (2): ConvBottleneck(
      (seq): Sequential(
        (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): ReLU(inplace=True)
      )
    )
    (3): ConvBottleneck(
      (seq): Sequential(
        (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (1): ReLU(inplace=True)
      )
    )
  )
  (decoder_stages): ModuleList(
    (0): UnetDecoderBlock(
      (layer): Sequential(
        (0): Upsample(scale_factor=2.0, mode=nearest)
        (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (2): ReLU(inplace=True)
      )
    )
    (1): UnetDecoderBlock(
      (layer): Sequential(
        (0): Upsample(scale_factor=2.0, mode=nearest)
        (1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (2): ReLU(inplace=True)
      )
    )
    (2): UnetDecoderBlock(
      (layer): Sequential(
        (0): Upsample(scale_factor=2.0, mode=nearest)
        (1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (2): ReLU(inplace=True)
      )
    )
    (3): UnetDecoderBlock(
      (layer): Sequential(
        (0): Upsample(scale_factor=2.0, mode=nearest)
        (1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (2): ReLU(inplace=True)
      )
    )
  )
  (last_upsample): UnetDecoderBlock(
    (layer): Sequential(
      (0): Upsample(scale_factor=2.0, mode=nearest)
      (1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (2): ReLU(inplace=True)
    )
  )
  (final): Sequential(
    (0): Conv2d(32, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  )
  (encoder_stages): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
    )
    (1): Sequential(
      (0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (1): Sequential(
        (0): BasicBlock(
          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (1): BasicBlock(
          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
        (2): BasicBlock(
          (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
    )
    (2): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): BasicBlock(
        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (3): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (3): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (4): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (5): BasicBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (4): Sequential(
      (0): BasicBlock(
        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): BasicBlock(
        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
      (2): BasicBlock(
        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
)
06-24 16:25 log      INFO     train epoch 60, time elapsed (minutes): 10.1
06-24 16:25 log      INFO       train epoch 60, train loss: {'tot_loss': 0.1288240486942571, 'focal': 0.0221361367096395, 'dice_loss': 0.47669954530094055, 'mse': 0.06435101554103073} 
06-24 16:25 log      INFO       train epoch 60, val loss: {'tot_loss': 0.1655255469583696, 'focal': 0.032186065951537654, 'dice_loss': 0.5228063394946437, 'mse': 0.0834986368091995} 
06-24 16:25 log      INFO       train epoch 60, total time elapsed (minutes): 10.1
06-24 16:35 log      INFO     train epoch 61, time elapsed (minutes): 10.0
06-24 16:35 log      INFO       train epoch 61, train loss: {'tot_loss': 0.12884801651990696, 'focal': 0.021895585629159418, 'dice_loss': 0.4775841659535476, 'mse': 0.06386633139724533} 
06-24 16:35 log      INFO       train epoch 61, val loss: {'tot_loss': 0.1661217158359866, 'focal': 0.0322136513353858, 'dice_loss': 0.52821590323602, 'mse': 0.08297543483426213} 
06-24 16:35 log      INFO       train epoch 61, total time elapsed (minutes): 20.1
06-24 16:45 log      INFO     train epoch 62, time elapsed (minutes): 10.0
06-24 16:45 log      INFO       train epoch 62, train loss: {'tot_loss': 0.12867537158488793, 'focal': 0.021895692493123613, 'dice_loss': 0.4822701892131882, 'mse': 0.06347569961436704} 
06-24 16:45 log      INFO       train epoch 62, val loss: {'tot_loss': 0.16615877031318604, 'focal': 0.03264130950513716, 'dice_loss': 0.5312856251193631, 'mse': 0.08303736547382375} 
06-24 16:45 log      INFO       train epoch 62, total time elapsed (minutes): 30.2
06-24 16:55 log      INFO     train epoch 63, time elapsed (minutes): 10.0
06-24 16:55 log      INFO       train epoch 63, train loss: {'tot_loss': 0.12961282562091267, 'focal': 0.0220590471243401, 'dice_loss': 0.492547111107352, 'mse': 0.06391686875413839} 
06-24 16:55 log      INFO       train epoch 63, val loss: {'tot_loss': 0.16552439846338765, 'focal': 0.03211879960700792, 'dice_loss': 0.553780637633416, 'mse': 0.08412877043301528} 
06-24 16:55 log      INFO       train epoch 63, total time elapsed (minutes): 40.2
06-24 17:05 log      INFO     train epoch 64, time elapsed (minutes): 10.0
06-24 17:05 log      INFO       train epoch 64, train loss: {'tot_loss': 0.1282505771121475, 'focal': 0.021964809486221773, 'dice_loss': 0.49749146122315757, 'mse': 0.06369954001630075} 
06-24 17:05 log      INFO       train epoch 64, val loss: {'tot_loss': 0.16593622204757505, 'focal': 0.032539753770291266, 'dice_loss': 0.5405160365566131, 'mse': 0.08443645848543732} 
06-24 17:05 log      INFO       train epoch 64, total time elapsed (minutes): 50.2
06-24 17:15 log      INFO     train epoch 65, time elapsed (minutes): 10.0
06-24 17:15 log      INFO       train epoch 65, train loss: {'tot_loss': 0.1280239858208241, 'focal': 0.02198605966722026, 'dice_loss': 0.488247403247326, 'mse': 0.0636799367524697} 
06-24 17:15 log      INFO       train epoch 65, val loss: {'tot_loss': 0.16635185763720542, 'focal': 0.0330141751627241, 'dice_loss': 0.5496688396699967, 'mse': 0.08357556103946867} 
06-24 17:15 log      INFO       train epoch 65, total time elapsed (minutes): 60.3
06-24 17:25 log      INFO     train epoch 66, time elapsed (minutes): 10.0
06-24 17:25 log      INFO       train epoch 66, train loss: {'tot_loss': 0.12775301133945774, 'focal': 0.021547182324428947, 'dice_loss': 0.4920825158094882, 'mse': 0.062374194914335976} 
06-24 17:25 log      INFO       train epoch 66, val loss: {'tot_loss': 0.16688603677095906, 'focal': 0.03415251064349943, 'dice_loss': 0.5320634968819157, 'mse': 0.08418149778909428} 
06-24 17:25 log      INFO       train epoch 66, total time elapsed (minutes): 70.3
06-24 17:35 log      INFO     train epoch 67, time elapsed (minutes): 10.0
06-24 17:35 log      INFO       train epoch 67, train loss: {'tot_loss': 0.12819987264052984, 'focal': 0.02180963607406573, 'dice_loss': 0.4842141324922252, 'mse': 0.06305830565218445} 
06-24 17:35 log      INFO       train epoch 67, val loss: {'tot_loss': 0.16550452987993916, 'focal': 0.03285577945083315, 'dice_loss': 0.5247614149124392, 'mse': 0.08451999329379171} 
06-24 17:35 log      INFO       train epoch 67, total time elapsed (minutes): 80.3
06-24 17:45 log      INFO     train epoch 68, time elapsed (minutes): 10.0
06-24 17:45 log      INFO       train epoch 68, train loss: {'tot_loss': 0.12770616237365917, 'focal': 0.02187775574600789, 'dice_loss': 0.4822925580003874, 'mse': 0.06344301263396314} 
06-24 17:45 log      INFO       train epoch 68, val loss: {'tot_loss': 0.16574494833907774, 'focal': 0.03203708242030978, 'dice_loss': 0.5267998499255027, 'mse': 0.08219195665683508} 
06-24 17:45 log      INFO       train epoch 68, total time elapsed (minutes): 90.4
06-24 17:55 log      INFO     train epoch 69, time elapsed (minutes): 10.0
06-24 17:55 log      INFO       train epoch 69, train loss: {'tot_loss': 0.1270599594858826, 'focal': 0.021562437271573774, 'dice_loss': 0.5028527887141119, 'mse': 0.06222461105132635} 
06-24 17:55 log      INFO       train epoch 69, val loss: {'tot_loss': 0.16578763656077847, 'focal': 0.032456234946695486, 'dice_loss': 0.5249049525107107, 'mse': 0.08376072073182848} 
06-24 17:55 log      INFO       train epoch 69, total time elapsed (minutes): 100.4
06-24 17:55 log      INFO     Time to train: 6033.33838224411 seconds
